<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Memory bandwidth has always been a challenge for video cards, and that challenge only continues to get harder. Thanks to the mechanics of Moores Law, GPU transistor counts  and therefore the quantities of various cores  is growing at a rapid pace. Meanwhile DRAM, whose bandwidth is not subject to the same laws, has"><meta name=generator content="Hugo 0.98.0"><meta name=robots content="index,follow,noarchive"><title>Feeding the Beast (2018): GDDR6 &amp;amp; Memory Compression &#183;</title><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css><!--[if lte IE 8]><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css><![endif]--><!--[if gt IE 8]><!--><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css><!--<![endif]--><!--[if lte IE 8]><link rel=stylesheet href=https://assets.cdnweb.info/hugo/blackburn/css/side-menu-old-ie.css><![endif]--><!--[if gt IE 8]><!--><link rel=stylesheet href=https://assets.cdnweb.info/hugo/blackburn/css/side-menu.css><!--<![endif]--><link rel=stylesheet href=https://assets.cdnweb.info/hugo/blackburn/css/blackburn.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=Raleway&display=swap" rel=stylesheet type=text/css><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/styles/androidstudio.min.css><script async src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/highlight.min.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel="shortcut icon" href=./img/favicon.ico type=image/x-icon></head><body><div id=layout><a href=#menu id=menuLink class=menu-link><span></span></a><div id=menu><a class="pure-menu-heading brand" href=./index.html>JiveVlog</a><div class=pure-menu><ul class=pure-menu-list><li class=pure-menu-item><a class=pure-menu-link href=./index.html><i class="fa fa-home fa-fw"></i>Home</a></li><li class=pure-menu-item><a class=pure-menu-link href=./post/index.html><i class="fa fa-list fa-fw"></i>Posts</a></li><li class=pure-menu-item><a class=pure-menu-link href=./sitemap.xml><i class="fa fa-user fa-fw"></i>Sitemap</a></li><li class=pure-menu-item><a class=pure-menu-link href=./index.xml><i class="fa fa-phone fa-fw"></i>RSS</a></li></ul></div><div class="pure-menu social"><ul class=pure-menu-list></ul></div><div><div class=small-print><small>&copy; 2022. All rights reserved.</small></div><div class=small-print><small>Built with&nbsp;<a href=https://gohugo.io/ target=_blank>Hugo</a></small>
<small>Theme&nbsp;<a href=https://github.com/yoshiharuyamashita/blackburn target=_blank>Blackburn</a></small></div></div></div><div id=main><div class=header><h1>Feeding the Beast (2018): GDDR6 &amp;amp; Memory Compression</h1><h2>Memory bandwidth has always been a challenge for video cards, and that challenge only continues to get harder. Thanks to the mechanics of Moores Law, GPU transistor counts and therefore the quantities of various cores is growing at a rapid pace. Meanwhile DRAM, whose bandwidth is not subject to the same laws, has</h2></div><div class=content><div class=post-meta><div><i class="fa fa-calendar fa-fw"></i>
<time>12 Sep 2024, 00:00</time></div></div><p>Memory bandwidth has always been a challenge for video cards, and that challenge only continues to get harder. Thanks to the mechanics of Moore’s Law, GPU transistor counts – and therefore the quantities of various cores – is growing at a rapid pace. Meanwhile DRAM, whose bandwidth is not subject to the same laws, has grown at a much smaller pace.</p><p>The net result is that with nearly every generation, the amount of memory bandwidth available per FLOP, per texture lookup, and per pixel blend has continued to drop. So to keep GPU performance scaling – to feed the great graphical beast – GPU manufacturers and the memory industry as a whole have to look for new ways to boost memory bandwidth for future memory technologies while reducing the amount of memory bandwidth they’re using right now. Neither is easy, and both are areas where NVIDIA has been executing well on over most of the past decade, making it an architectural strength for the company.</p><table align=center border=0 cellpadding=0 cellspacing=1 width=575><tbody readability=1><tr class=tgrey readability=2><td align=center colspan=7>NVIDIA Memory Bandwidth per FLOP (In Bits)</td></tr><tr class=tlblue><td align=center class=tlgrey valign=middle width=190>GPU</td><td align=center valign=middle width=190>Bandwidth/FLOP</td><td align=center valign=middle width=190>Total CUDA FLOPs</td><td align=center valign=middle width=191>Total Bandwidth</td></tr><tr><td align=center class=tlgrey valign=middle>RTX 2080</td><td align=center valign=middle>0.36 bits</td><td align=center valign=middle>10.06 TFLOPs</td><td align=center valign=middle>448GB/sec</td></tr><tr><td align=center class=tlgrey valign=middle>GTX 1080</td><td align=center valign=middle>0.29 bits</td><td align=center valign=middle>8.87 TFLOPs</td><td align=center valign=middle>320GB/sec</td></tr><tr><td align=center class=tlgrey valign=middle>GTX 980</td><td align=center valign=middle>0.36 bits</td><td align=center valign=middle>4.98 TFLOPs</td><td align=center valign=middle>224GB/sec</td></tr><tr><td align=center class=tlgrey valign=middle>GTX 680</td><td align=center valign=middle>0.47 bits</td><td align=center valign=middle>3.25 TFLOPs</td><td align=center valign=middle>192GB/sec</td></tr><tr><td align=center class=tlgrey valign=middle>GTX 580</td><td align=center valign=middle>0.97 bits</td><td align=center valign=middle>1.58 TFLOPs</td><td align=center valign=middle>192GB/sec</td></tr></tbody></table><p>Turing, in turn, is a bit of an interesting swerve in this pattern thanks to its heavy focus on ray tracing and neural network inferencing. If we're looking at memory bandwidth merely per CUDA core FLOP, then bandwidth per FLOP has actually gone up, since RTX 2080 doesn't deliver a significant increase in (on-paper) CUDA core throughput relative to GTX 1080. However RTX 2080 also now has to feed ray tracing cores and tensor cores, both of which are very bandwidth hungry on their own. So while in pure, FP32 core compute scenarios the situation has improved a bit, once the entire GPU is put to work, the amount of contention for memory bandwidth is still higher than ever.</p><p>In terms of memory technologies, the 16nm/14nm generation of GPUs saw an interesting and atypical divergence in the memory space. GDDR5, which has been with us for a decade now, has been ripe for replacement. The JEDEC, the industry standardization body responsible for setting common memory standards, initially approached this in two directions. The first route was more traditional, developing a successor technology to GDDR5, which became GDDR5X. Meanwhile the more radical approach was ultra-wide I/O technologies, which became the High Bandwidth Memory (HBM) standards.</p><p>NVIDIA for their part embraced both, but at different levels. HBM is very powerful, but the realities of wide I/O make it harder to manufacture and more costly to put on a product, thanks to the need for a silicon interposer. As a result, HBM (and specifically, HBM2) has only ever been used on NVIDIA’s flagship compute GPUs, the GP100 and GV100. For everything else, NVIDIA turned to GDDR5X. And this is where things get a bit odd.</p><p>GDDR5X is a JEDEC standard like GDDR5 before it, but it simply never saw the same kind of adoption that GDDR5 did. This goes both for memory vendors and GPU vendors. Only Micron ever produced the memory, and only NVIDIA ever used it. So the fastest Pascal cards – GTX 1080, GTX 1080 Ti, & Titan Xp – are outliers in that they’re the only (consumer) cards using this memory technology. GDDR5X was an important piece of the Pascal puzzle as it allowed NVIDIA to better feed their fastest cards, but in time it has essentially became a dead-end branch of GDDR memory technology as it never saw the kind of adoption required to reach critical mass.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/13282/Samsung_GDDR6_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>So where did GDDR branch to instead? This brings us to GDDR6, the latest and greatest in GDDR memory technology. And unlike GDDR5X before it, GDDR6 has the full backing of the Big 3 memory manufacturers – Samsung, SK Hynix, and Micron – so the memory industry as a whole has a much larger stake in the technology. For NVIDIA’s products, this is evident right off the bat: NVIDIA is using Samsung’s 16Gb capacity GDDR6 modules for their Quadro cards, and meanwhile they’re tapping Micron’s 8Gb modules for the new GeForce RTX cards.</p><p>The performance impact of GDDR6, in turn, depends in part on what we’re comparing it to. Relative to GDDR5X, GDDR6 is not quite as big of a step up as some past memory generations, as many of GDDR6’s innovations were already baked into GDDR5X. GDDR5 officially topped out at 8Gbps per pin (with NV working with partners to do 9Gbps overclocked SKUs), while NVIDIA shipped GDDR5X cards clocked as high as 11.4Gbps. GDDR6, in turn, is going to be starting at 14Gbps in graphics cards, with future generations of the technology set to reach 16Gbps and higher. So for the likes of NVIDIA’s x70 cards, the switch from GDDR5 to GDDR6 is going to be one of those massive once-in-a-generation bandwidth jumps. However for NVIDIA’s x80 cards, the upgrade from GDDR5X to GDDR6 is going to give those products a healthy increase in memory bandwidth, it just won’t be a huge jump.</p><p>Diving a bit deeper here, there are really two core changes coming from GDDR5 that enable GDDR6’s big bandwidth boost. The first is the implementation of Quad Data Rate (QDR) signaling on the memory bus. Whereas GDDR5’s memory bus would transfer data twice per write clock (WCK) via DDR, GDDR6 (& 5X) extends this to four transfers per clock. All other things held equal, this allows GDDR6 to transfer twice as much data per clock as GDDR5.</p><p>The challenge in doing this, of course, is that the more you pump a memory bus, the tighter the signal integrity requirements. So while it’s simple to say “let’s just double the memory bus bandwidth”, doing it is another matter. In practice a lot of work goes into the GPU memory controller, the memory itself, and the PCB to handle these transmission speeds.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/13282/NVEye2_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Every time NVIDIA launches support for a new memory technology, they like to roll out a new “eye diagram” signal analysis graph. And while at a high level these things don’t tell us anything we don’t already know – that NVIDIA has the technology working, and that, if it wasn’t, they wouldn’t publish these – they’re none the less neat to see. In this case we’re looking at a fairly clean eye diagram, illustrating the very tight 70<span>p</span>s transitions between data transfers. NVIDIA says that they were able to reduce signal crosstalk by 40% here, which is one of the key signal integrity changes required to make GDDR6’s high speed signaling possible.</p><p>Moving on, the second big change for GDDR6 is that how data is read out of the DRAM cells themselves has changed. For many generations the solution has been to just read and write in larger strides – the prefetch value – with GDDR5 taking this to 8n and GDDR5X taking it to 16n. However the resulting access granularities of 32 bytes and 64 bytes respectively were on the path of becoming increasingly suboptimal for small memory operations. As a result, GDDR6 does a larger prefetch and yet it does not.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/13282/GDDR_Channels_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Whereas both GDDR5 and GDDR5X used a single 32-bit channel per chip, GDDR6 instead uses a pair of 16-bit channels. This means that in a single memory core clock cycle (ed: not to be confused with the memory bus), 32 bytes will be fetched from each channel for a total of 64 bytes. This means that each GDDR6 memory chip can fetch twice as much data per clock as a GDDR5 chip, but it doesn’t have to be one contiguous chunk of memory. In essence, each GDDR6 memory chip can function like two chips.</p><p>For graphics this doesn’t have much of an impact since GPUs already read and write to RAM in massive sequential parallelism. However it’s a more meaningful change for other markets. In this case the smaller memory channels will help with random access performance, especially compared to GDDR5X and its massive 64 byte access granularity.</p><p>Moving on, GDDR6 also implements some changes to further reduce power consumption – or perhaps it’s better to say that these keep power consumption from continuing to grow. The standard operating voltage for the memory technology is 1.35v; this is identical to GDDR5X’s 1.35v voltage, but down from 1.5v for standard GDDR5.</p><p>The actual power savings are a bit hard to quantify here, as NVIDIA has rolled that data into all of their memory controller optimizations. But at least publicly, what they are saying is that in conjunction with “extensive” clock gating, they’ve been able to improve power efficiency by 20% over Pascal and GDDR5X, and undoubtedly by more versus Pascal paired with GDDR5. That said, these numbers should be taken with a small grain of salt, as these numbers appear to be averages rather than peaks. NVIDIA’s clock gating enhancements are primarily about reducing power consumption when GDDR6 is not running at full utilization, so peak power may be another story.</p><table align=center border=0 cellpadding=0 cellspacing=1 width=670><tbody readability=4><tr class=tgrey readability=2><td align=center colspan=9>GPU Memory Math: GDDR6 vs. HBM2 vs. GDDR5X</td></tr><tr class=tlblue readability=6><td width=186>&nbsp;</td><td align=center valign=middle width=137>NVIDIA GeForce RTX 2080 Ti<br>(GDDR6)</td><td align=center valign=middle width=136>NVIDIA GeForce RTX 2080<br>(GDDR6)</td><td align=center valign=middle width=136>NVIDIA Titan V<br>(HBM2)</td><td align=center valign=middle width=136>NVIDIA Titan Xp<br>&nbsp;</td><td align=center valign=middle width=136>NVIDIA GeForce GTX 1080 Ti</td><td align=center valign=middle width=136>NVIDIA GeForce GTX 1080</td></tr><tr><td class=tlgrey>Total Capacity</td><td align=center valign=middle>11 GB</td><td align=center valign=middle>8 GB</td><td align=center valign=middle>12 GB</td><td align=center valign=middle>12 GB</td><td align=center valign=middle>11 GB</td><td align=center valign=middle>8 GB</td></tr><tr><td class=tlgrey>B/W Per Pin</td><td align=center colspan=2 rowspan=1 valign=middle>14 Gb/s</td><td align=center valign=middle>1.7 Gb/s</td><td align=center valign=middle>11.4 Gbps</td><td align=center colspan=2 rowspan=1 valign=middle>11 Gbps</td></tr><tr><td class=tlgrey>Chip capacity</td><td align=center colspan=2 rowspan=1 valign=middle>1 GB (8 Gb)</td><td align=center valign=middle>4&nbsp;GB&nbsp;(32&nbsp;Gb)</td><td align=center colspan=3 rowspan=1 valign=middle>1 GB (8 Gb)</td></tr><tr><td class=tlgrey>No. Chips/KGSDs</td><td align=center valign=middle>11</td><td align=center valign=middle>8</td><td align=center valign=middle>3</td><td align=center valign=middle>12</td><td align=center valign=middle>11</td><td align=center valign=middle>8</td></tr><tr><td class=tlgrey>B/W Per Chip/Stack</td><td align=center colspan=2 rowspan=1 valign=middle>56 GB/s</td><td align=center valign=middle>217.6&nbsp;GB/s</td><td align=center valign=middle>45.6 GB/s</td><td align=center colspan=2 rowspan=1 valign=middle>44 GB/s</td></tr><tr><td class=tlgrey>Bus Width</td><td align=center valign=middle>352-bit</td><td align=center valign=middle>256-bit</td><td align=center valign=middle>3092-bit</td><td align=center valign=middle>384-bit</td><td align=center valign=middle>352-bit</td><td align=center valign=middle>256-bit</td></tr><tr><td class=tlgrey>Total B/W</td><td align=center valign=middle>616 GB/s</td><td align=center valign=middle>448GB/s</td><td align=center valign=middle>652.8&nbsp;GB/s</td><td align=center valign=middle>547.7&nbsp;GB/s</td><td align=center valign=middle>484&nbsp;GB/s</td><td align=center valign=middle>352&nbsp;GB/s</td></tr><tr><td class=tlgrey>DRAM Voltage</td><td align=center colspan=2 rowspan=1 valign=middle>1.35 V</td><td align=center valign=middle>1.2 V (?)</td><td align=center colspan=3 rowspan=1 valign=middle>1.35 V</td></tr></tbody></table><p>All told then, NVIDIA will be the first GPU manufacturer to roll out GDDR6 support. And with the GTX 2070 on-up having all been announced already, it’s already going to be a wider roll-out than what we saw for GDDR5X. And to put things in numbers, relative to the GTX 10 series, the RTX 2080 Ti will get 27% more memory bandwidth, the RTX 2080 40% more bandwidth, and the RTX 2070 a whopping 75% more memory bandwidth than its predecessor.</p><p>However as this is a brand-new memory technology, I’m not sure whether we’re going to see it on the obligatory 2060 & 2050 cards. In transition periods, these tiers have been known to use older memory for cost and supply reasons – so we’ll have to see what happens.</p><p>Finally, just as GDDR6 is already seeing greater adoption on the memory manufacturer side, I’m expecting the same on the GPU side. AMD hasn’t announced their plans thus far, but I will be greatly surprised if we see them skip out on GDDR6 like they did GDDR5X.</p><h3>Turing: Memory Compression Iterated</h3><p>As I stated at the start of this section, the key to improving GPUs is to tackle the problem from two directions: increase the available memory bandwidth, and then decrease how much of it you use. For the latter, NVIDIA has employed a number of tricks over the years. Perhaps the most potent of which (that they’re willing to talk about, at least) being their memory compression technology.</p><p>The cornerstone of memory compression is what is called data color compression. DCC is a per-buffer/per-frame compression method that breaks down a frame into tiles, and then looks at the differences between neighboring pixels – their deltas. By utilizing a large pattern library, NVIDIA is able to try different patterns to describe these deltas in as few pixels as possible, ultimately conserving bandwidth throughout the GPU, not only reducing DRAM bandwidth needs, but also L2 bandwidth needs and texture unit bandwidth needs (in the case of reading back a compressed render target).</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/13282/PascalEdDay_FINAL_NDA_1463156837-008_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>With Pascal, NVIDIA rolled out their 4th generation technology, and now with Turing we’re at the 5th generation. Unfortunately, details on what the 5th generation entails are very slim; NVIDIA just isn’t talking about the technology much. The nature of DCC is such that it’s meant to be expandable: more silicon can be devoted to allowing more patterns to be checked at once. So it’s practically guaranteed that NVIDIA has once again expanded their library of patterns. However what those expanded patterns are, we don’t know.</p><p>However of the limited information NVIDIA has released, they’ve offered some effective memory bandwidth graphs, with the results broken down into how much of that gain came from actual memory bandwidth increases, and then how much of that came from efficiency increases. In NVIDIA’s examples the effective increase in bandwidth varies by game; as this is the RTX 2080 Ti, GDDR6 provides a constant 27% of the improvement, while the rest of the improvement is variable based on how useful NVIDIA’s memory compression updates are to the given game.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/13282/NV_Turing_Editors_Day_015_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Overall, NVIDIA is seeing anywhere between a 45% increase in effective memory bandwidth for Grand Theft Auto V, up to a 60% increase for Ashes of the Singularity. Which, if we subtract out the base 27% memory clock increase, means that the efficiency increases are between 18% and 33%.</p><p>More broadly speaking, NVIDIA is claiming a 50% increase in effective memory bandwidth for the RTX 2080 Ti versus the GTX 1080 Ti. Which again subtracting the base 27% memory bandwidth increase from GDDR6, leaves us with an average efficiency improvement of 23%.</p><p>Relative to Pascal then, this is a smaller increase in effective memory bandwidth, but a slightly larger increase in compression efficiency. For Pascal – and specifically, GTX 1080 – NVIDIA claimed a 70% effective memory bandwidth increase, of which 20% was compression improvements.</p><p>So while NVIDIA isn’t gaining as much effective memory bandwidth this time around due to the smaller step up from GDDR5X to GDDR6, their compression gains have actually improved a bit between generations. Which is actually a bit surprising, as I would have otherwise expected diminishing returns in the gains from memory compression. After all, NVIDIA started with the most commonly seen pixel patterns, and each generation of DCC would be adding less common patterns.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH50fpdrZqeumZm2onnTrqmippdirrOvx6Krnpukqr%2BmecOenKlllJ7DpnuX</p><h4><i class="fas fa-share-alt" aria-hidden=true></i>&nbsp;Share!</h4><ul class=share-buttons><li><a href="https://www.facebook.com/sharer/sharer.php?u=%2fnvidia-turing-architecture-deep-dive.html" target=_blank title="Share on Facebook"><i class="fab fa-facebook" aria-hidden=true></i><span class=sr-only>Share on Facebook</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="https://twitter.com/intent/tweet?source=%2fnvidia-turing-architecture-deep-dive.html" target=_blank title=Tweet><i class="fab fa-twitter" aria-hidden=true></i><span class=sr-only>Tweet</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="https://plus.google.com/share?url=%2fnvidia-turing-architecture-deep-dive.html" target=_blank title="Share on Google+"><i class="fab fa-google-plus" aria-hidden=true></i><span class=sr-only>Share on Google+</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="http://www.tumblr.com/share?v=3&u=%2fnvidia-turing-architecture-deep-dive.html" target=_blank title="Post to Tumblr"><i class="fab fa-tumblr" aria-hidden=true></i><span class=sr-only>Post to Tumblr</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="http://pinterest.com/pin/create/button/?url=%2fnvidia-turing-architecture-deep-dive.html" target=_blank title="Pin it"><i class="fab fa-pinterest-p" aria-hidden=true></i><span class=sr-only>Pin it</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="http://www.reddit.com/submit?url=%2fnvidia-turing-architecture-deep-dive.html" target=_blank title="Submit to Reddit"><i class="fab fa-reddit-alien" aria-hidden=true></i><span class=sr-only>Submit to Reddit</span></a></li></ul><style>ul.share-buttons{list-style:none;padding:0}ul.share-buttons li{display:inline}ul.share-buttons .sr-only{position:absolute;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);padding:0;border:0;height:1px;width:1px;overflow:hidden}</style><div class="prev-next-post pure-g"><div class=pure-u-1-24 style=text-align:left><a href=./shefali-bhavsar-height-weight-net-worth-age-birthday-wikipedia-who-instagram-biography-604053-html.html><i class="fa fa-chevron-left"></i></a></div><div class=pure-u-10-24><nav class=prev><a href=./shefali-bhavsar-height-weight-net-worth-age-birthday-wikipedia-who-instagram-biography-604053-html.html>Shefali Bhavsar Height, Weight, Net Worth, Age, Birthday, Wikipedia, Who, Instagram, Biography</a></nav></div><div class=pure-u-2-24>&nbsp;</div><div class=pure-u-10-24><nav class=next><a href=./all-of-taylor-swifts-boyfriends-in-order-chronologically.html>All of Taylor Swifts boyfriends in order, chronologically</a></nav></div><div class=pure-u-1-24 style=text-align:right><a href=./all-of-taylor-swifts-boyfriends-in-order-chronologically.html><i class="fa fa-chevron-right"></i></a></div></div></div></div></div><script src=https://assets.cdnweb.info/hugo/blackburn/js/ui.js></script>
<script src=https://assets.cdnweb.info/hugo/blackburn/js/menus.js></script>
<script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>